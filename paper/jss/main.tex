\documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------


%% recommended packages
\usepackage{thumbpdf,lmodern, amsmath, amssymb}
\usepackage[utf8]{inputenc}

%% another package (only for this demo article)
\usepackage{framed}

% package for cross and check
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% package for the table
\usepackage{geometry}
\usepackage{ragged2e}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{}
\usepackage{booktabs, makecell, tabularx}
\newcolumntype{b}{>{\RaggedRight}X}
\newcolumntype{s}{>{\hsize=0.2\hsize}X}


%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Leo Simpson \\TUM
   \And Patrick L. Combettes \\NC State
   \And Christian L. M\"uller\\LMU}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{c-lasso - a package for constrained sparse and robust regression and classification in \proglang{Python}}
\Plaintitle{c-lasso: sparse regression and classification}
\Shorttitle{The c-lasso package}

%% - \Abstract{} almost as usual
\Abstract{
  This article illustrates c-lasso, a Python package that enables sparse and robust linear
  regression and classification with linear equality constraints. The package handles several
  estimators for inferring location and scale, including the constrained Lasso, the constrained
  scaled Lasso, and sparse Huber M-estimation with linear equality constraints. Several
  algorithmic strategies, including path and proximal splitting algorithms, are implemented to
  solve the underlying convex optimization problems. We also include two model selection
  strategies for determining the sparsity of the model parameters: k-fold cross-validation and
  stability selection. This package is intended to fill the gap between popular python tools such
  as \pkg{scikit-learn} which \emph{cannot} solve sparse constrained problems and general-purpose
  optimization solvers such as \pkg{cvx} that do not scale well for the considered problems or are
  inaccurate. We show several use cases of the package, including an application of sparse
  log-contrast regression tasks for compositional microbiome data. We also highlight the seamless
  integration of the solver into \proglang{R} via the \pkg{reticulate} package. 
}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{constrained lasso, log-contrast model, sparse classification, \proglang{Python}}
\Plainkeywords{JSS, style guide, comma-separated, not capitalized, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\\).
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Christian L. M\"uller\\
  Department of Statistics\\
  Ludwig-Maximilians-Universit\"at M\"unchen\\
  Ludwigstrasse \\
  M\"unchen, Germany\\
  E-mail: \email{christian.mueller@stat.uni-muenchen.de}\\
  URL: \url{}\\
}

\begin{document}

%% -- Introduction -------------------------------------------------------------

%% - In principle "as usual".
%% - But should typically have some discussion of both _software_ and _methods_.
%% - Use \proglang{}, \pkg{}, and \code{} markup throughout the manuscript.
%% - If such markup is in (sub)section titles, a plain text version has to be
%%   added as well.
%% - All software mentioned should be properly \cite-d.
%% - All abbreviations should be introduced.
%% - Unless the expansions of abbreviations are proper names (like "Journal
%%   of Statistical Software" above) they should be in sentence case (like
%%   "generalized linear models" below).


\newpage
\section{Introduction} \label{sec:intro}

\subsection{Forward model} \label{sec:forward}

The forward model is assumed to be:
\begin{equation}
y = X \beta + \sigma \epsilon \qquad \textrm{s.t.} \qquad C\beta=0
\end{equation}

Here, $X$ and $y$ are given outcome and predictor data. The vector y can be continuous (for
regression) or binary (for classification). $C$ is a general constraint matrix. The vector
$\beta$ comprises the unknown coefficients and $\sigma$ an unknown scale \citep{}.

\subsection{Formulations} \label{sec:formulation}

Depending on the prior on the solution s$\beta, \sigma$ and on the noise $\epsilon$, the previous forward model can lead to different types of estimation problems. 

Our package can solve six of those : four regression-type and two classification-type formulations.

\paragraph{[R1] Standard constrained Lasso regression} 

\begin{equation*}
    \arg \min_{\beta \in \mathbb{R}^d} || X\beta - y ||^2 + \lambda \norm{\beta}_1 \qquad \mbox{s.t.} \qquad  C\beta = 0
\end{equation*}

This is the standard Lasso problem with linear equality constraints on the $\beta$ vector. The objective function combines Least-Squares for model fitting with l1 penalty for sparsity.

\paragraph{[R2] Contrained sparse Huber regression}

\begin{equation*}
    \arg \min_{\beta \in \mathbb{R}^d} h_{\rho} (X\beta - y) + \lambda \norm{\beta}_1 \qquad \mbox{s.t.} \qquad  C\beta = 0
\end{equation*}

This regression problem uses the Huber loss as objective function for robust model fitting with l1 and linear equality constraints on the $\beta$ vector. The parameter $\rho=1.345$.

\paragraph{[R3] Contrained scaled Lasso regression}

\begin{equation*}
    \arg \min_{\beta \in \mathbb{R}^d, \sigma > 0} \left( \norm{\frac{ X\beta - y }{\sigma}}^2 + \frac{n}{2} \right) \sigma+ \lambda \norm{\beta}_1 \qquad \mbox{s.t.} \qquad  C\beta = 0
\end{equation*}

This formulation is similar to [R1] but allows for joint estimation of the (constrained) $\beta$ vector and the standard deviation $\sigma$ in a concomitant fashion (see \cite{Compositional} or \cite{combettes2019regression} for further info). This is the default problem formulation in c-lasso.

\paragraph{[R4] Contrained sparse Huber regression with concomitant scale estimation}

\begin{equation*}
    \arg \min_{\beta \in \mathbb{R}^d, \sigma > 0} \left( h_{\rho} \left( \frac{ X\beta - y}{\sigma} \right)+ n \right) \sigma+ \lambda \norm{\beta}_1 \qquad \mbox{s.t.} \qquad  C\beta = 0
\end{equation*}

This formulation combines [R2] and [R3] to allow robust joint estimation of the (constrained) $\beta$ vector and the scale $\sigma$ in a concomitant fashion (see \cite{Compositional} or \cite{combettes2019regression} for further info).

\paragraph{[C1] Contrained sparse classification with Square Hinge loss}

\begin{equation*}
    \arg \min_{\beta \in \mathbb{R}^d} L(y^T X\beta - y) + \lambda \norm{\beta}_1 \qquad \mbox{s.t.} \qquad  C\beta = 0
\end{equation*}

where $L \left((r_1,...,r_n)^T \right) := \sum_{i=1}^n l(r_i)$ and $l$ is defined as :

\begin{equation*}
l(r) = \begin{cases} (1-r)^2 &\mbox{if } \leq r \leq 1 \\ 0 &\mbox{if } r \geq 1 \end{cases}
\end{equation*}


This formulation is similar to [R1] but adapted for classification tasks using the Square Hinge loss with (constrained) sparse $\beta$ vector estimation.

\paragraph{[C2] Contrained sparse classification with Huberized Square Hinge loss}

\begin{equation*}
    \arg \min_{\beta \in \mathbb{R}^d} L_{\rho}(y^T X\beta - y) + \lambda \norm{\beta}_1 \qquad \mbox{s.t.} \qquad  C\beta = 0
\end{equation*}

where $L_{\rho} \left((r_1,...,r_n)^T \right) := \sum_{i=1}^n l_{\rho}(r_i)$ and $l_{\rho}$ is defined as :

\begin{equation*}
l_{\rho}(r) = \begin{cases} (1-r)^2 &\mbox{if } \rho \leq r \leq 1 \\ (1-\rho)(1+\rho-2r) &\mbox{if } r \leq \rho \\ 0 &\mbox{if } r \geq 1 \end{cases}
\end{equation*}

This formulation is similar to [C1] but uses the Huberized Square Hinge loss for robust classification with (constrained) sparse $\beta$ vector estimation.

\subsection{Model selections} \label{sec:selection}




\section{Optimization algorithms} \label{sec:optimization}



\begin{table}[h!]
\centering
\begin{tabularx}{\textwidth}{bssssssb}
\toprule
\thead{Algorithm}
&   \thead{R1}
&   \thead{R2}
&   \thead{R3}
&   \thead{R4}
&   \thead{C1}
&   \thead{C2}
&   \thead{Description}             \\
\midrule
Path algorithms (Path-Alg)
& \cmark  & \cmark \quad \ref{sec:PA-R2} & \cmark \quad \ref{sec:PA-R3} & \cmark \quad \ref{sec:PA-R4} & \cmark & \cmark & \cite{Gaines}  \\ 


\addlinespace
Douglas-Rachford-type splitting method (DR)
& \cmark  & \cmark  & \cmark & \cmark & \xmark & \xmark &
\cite{Compositional}\\ 

\addlinespace
Projected primal-dual splitting method (P-PDS)
& \cmark  & \cmark  & \xmark & \xmark & \xmark & \xmark &
\cite{P+PDS} \\ 

\addlinespace
Projection-free primal-dual splitting method (PF-PDS) 
& \cmark  & \cmark & \xmark & \xmark & \xmark & \xmark &
\cite{PF+PDS}

\bottomrule
\end{tabularx}
\caption{\label{tab:overview} Overview of optimization schemes implemented}
\end{table}





Alternative reference  : \cite{Comlasso}


\newpage

\section{The python package}\label{sec:optimization}

\subsection{Installation}\label{sec:installation}

\subsection{Dependencies}\label{Dependencies}

\subsection{Defining the problem}\label{sec:problem}

\subsection{Solving the problem}\label{sec:solve}

\subsection{Visualisations}\label{sec:visualisations}



\newpage
\bibliography{refs}




\begin{appendix}

\section{More technical details} \label{app:technical}


\subsection{Adaption of Path-Alg for R2} \label{sec:PA-R2}
\subsection{Adaptation of Path-Alg for R3} \label{sec:PA-R3}
\subsection{Adaptation of Path-Alg for R4} \label{sec:PA-R4}




\end{appendix}



\end{document}
